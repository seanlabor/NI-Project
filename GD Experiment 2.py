# -*- coding: utf-8 -*-
"""Creating a neural network in JAX.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/nahumsa/JAX/blob/master/Simple%20NN%20JAX.ipynb
"""

# -*- coding: utf-8 -*-
"""arch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/d2l-ai/d2l-tvm-colab/blob/master/chapter_gpu_schedules/arch.ipynb
"""

!pip install -q flax

import jax
import jax.numpy as jnp                # JAX NumPy

from flax import linen as nn           # The Linen API
from flax.training import train_state  # Useful dataclass to keep train state

import numpy as np                     # Ordinary NumPy
import optax                           # Optimizers
import tensorflow_datasets as tfds     # TFDS for MNIST

def cross_entropy_loss(*, logits, labels):
  labels_onehot = jax.nn.one_hot(labels, num_classes=num_classes)
  return optax.softmax_cross_entropy(logits=logits, labels=labels_onehot).mean()

def compute_metrics(*, logits, labels):
  loss = cross_entropy_loss(logits=logits, labels=labels)
  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)
  metrics = {
      'loss': loss,
      'accuracy': accuracy,
  }
  return metrics

def create_train_state(rng, learning_rate, momentum):
  """Creates initial `TrainState`."""
  cnn = CNN()
  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']
  tx = optax.sgd(learning_rate, momentum)
  return train_state.TrainState.create(
      apply_fn=cnn.apply, params=params, tx=tx)

@jax.jit
def train_step(state, batch):
  """Train for a single step."""
  def loss_fn(params):
    logits = CNN().apply({'params': params}, batch['image'])
    loss = cross_entropy_loss(logits=logits, labels=batch['label'])
    return loss, logits
  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
  (_, logits), grads = grad_fn(state.params)
  state = state.apply_gradients(grads=grads)
  metrics = compute_metrics(logits=logits, labels=batch['label'])
  return state, metrics

@jax.jit
def eval_step(params, batch):
  logits = CNN().apply({'params': params}, batch['image'])
  return compute_metrics(logits=logits, labels=batch['label'])

class CNN(nn.Module):
  """A simple CNN model."""

  @nn.compact
  def __call__(self, x):
    x = nn.Conv(features=32, kernel_size=(3, 3))(x)
    x = nn.relu(x)
    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
    x = nn.Conv(features=64, kernel_size=(3, 3))(x)
    x = nn.relu(x)
    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
    x = x.reshape((x.shape[0], -1))  # flatten
    x = nn.Dense(features=50)(x)
    x = nn.relu(x)
    x = nn.Dense(features=num_classes)(x)
    
    return x

def train_epoch(state, train_ds, batch_size, epoch, rng):
  """Train for a single epoch."""
  train_ds_size = len(train_ds['image'])
  steps_per_epoch = train_ds_size // batch_size

  perms = jax.random.permutation(rng, train_ds_size)
  perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch
  perms = perms.reshape((steps_per_epoch, batch_size))
  batch_metrics = []
  for perm in perms:
    batch = {k: v[perm, ...] for k, v in train_ds.items()}
    state, metrics = train_step(state, batch)
    batch_metrics.append(metrics)

  # compute mean of metrics across each batch in epoch.
  batch_metrics_np = jax.device_get(batch_metrics)
  epoch_metrics_np = {
      k: np.mean([metrics[k] for metrics in batch_metrics_np])
      for k in batch_metrics_np[0]}

  #print('train epoch: %d, loss: %.4f, accuracy: %.2f' % (
      #epoch, epoch_metrics_np['loss'], epoch_metrics_np['accuracy'] * 100))

  return state

def eval_model(params, test_ds):
  metrics = eval_step(params, test_ds)
  metrics = jax.device_get(metrics)
  summary = jax.tree_util.tree_map(lambda x: x.item(), metrics)
  return summary['loss'], summary['accuracy']

num_classes=10

from numpy.random import default_rng

def get_datasets_allmnist(random_categories,seed=0):
  rng_idx = default_rng(seed)

  
  map_dic = dict(zip(random_categories, list(range(num_classes))))


  """Load MNIST train and test datasets into memory."""
  ds_builder = tfds.builder("emnist/balanced")
  #ds_builder = tfds.builder("mnist")
  ds_builder.download_and_prepare()
  train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))
  test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))

  train_ds['image'] = train_ds['image'][np.isin(train_ds['label'], random_categories)]
  train_ds['label'] = train_ds['label'][np.isin(train_ds['label'], random_categories)]

  train_ds['label'] = np.vectorize(map_dic.get)(np.array(train_ds['label']))
  

  
  #random_idx = rng_idx.choice(len(train_ds['label']), size=n_samples, replace=False)

  #train_ds['image'] = train_ds['image'][random_idx]
  #train_ds['label'] = train_ds['label'][random_idx]

  test_ds['image'] = test_ds['image'][np.isin(test_ds['label'], random_categories)]
  test_ds['label'] = test_ds['label'][np.isin(test_ds['label'], random_categories)]

  test_ds['label'] = np.vectorize(map_dic.get)(np.array(test_ds['label']))

  train_ds['image'] = jnp.float32(train_ds['image']) / 255.
  test_ds['image'] = jnp.float32(test_ds['image']) / 255.
  return train_ds, test_ds

rng = jax.random.PRNGKey(0)
rng, init_rng = jax.random.split(rng)
learning_rate = 0.1
momentum = 0.9
batch_size=25
n_runs=10
state = create_train_state(init_rng, learning_rate, momentum)

del init_rng  # Must not be used anymore.

a = np.arange(0,46)


random_categories_train=[]
random_categories_test=[]
random_categories_train_list=[]
for num in a:
    if num % 2 == 0:
        random_categories_test.append(num)
    else:
        random_categories_train.append(num)

for m in range(n_runs):
  np.random.seed(m)
  np.random.shuffle(random_categories_train)
  random_categories_train_list.append(random_categories_train[:num_classes])

  print("random train categories: ",random_categories_train[:num_classes])

save_state_list=[]
for m in range(n_runs):
  rng = jax.random.PRNGKey(m)
  
  state = create_train_state(rng, learning_rate, momentum)
  train_ds, test_ds = get_datasets_allmnist(random_categories_train_list[m])
  for epoch in range(10):
    rng = jax.random.PRNGKey(epoch)
    # Use a separate PRNG key to permute image data during shuffling
    rng, input_rng = jax.random.split(rng)
    # Run an optimization step over a training batch
    state = train_epoch(state, train_ds, batch_size, epoch, input_rng)
    # Evaluate on the test set after each training epoch 
  test_loss, test_accuracy = eval_model(state.params, test_ds)
  print(' test epoch: %d, loss: %.2f, accuracy: %.2f' % (epoch, test_loss, test_accuracy * 100))

  save_state_list.append(state)

from numpy.random import default_rng

def get_datasets_unknown_allmnist(random_categories_test,seed):
  rng_idx = default_rng()

  np.random.seed(seed)
  np.random.shuffle(random_categories_test)
  random_categories = random_categories_test[:num_classes]
  #print("random_categories:",random_categories)

  map_dic = dict(zip(random_categories, list(range(num_classes))))


  """Load MNIST train and test datasets into memory."""
  ds_builder = tfds.builder("emnist/balanced")
  #ds_builder = tfds.builder("mnist")
  ds_builder.download_and_prepare()
  train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))
  test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))

  train_ds['image'] = train_ds['image'][np.isin(train_ds['label'], random_categories)]
  train_ds['label'] = train_ds['label'][np.isin(train_ds['label'], random_categories)]
  train_ds['label'] = np.vectorize(map_dic.get)(np.array(train_ds['label']))
  random_idx = rng_idx.choice(len(train_ds['label']), size=n_samples, replace=False)

  train_ds['image'] = train_ds['image'][random_idx]
  train_ds['label'] = train_ds['label'][random_idx]

  test_ds['image'] = test_ds['image'][np.isin(test_ds['label'], random_categories)]
  test_ds['label'] = test_ds['label'][np.isin(test_ds['label'], random_categories)]

  test_ds['label'] = np.vectorize(map_dic.get)(np.array(test_ds['label']))

  train_ds['image'] = jnp.float32(train_ds['image']) / 255.
  test_ds['image'] = jnp.float32(test_ds['image']) / 255.
  return train_ds, test_ds

'''Few Shot learning gradient one network - relearn trained network'''
learning_rate = 0.1
momentum = 0.9
batch_size=5

n_samples=50
megarun_acc_no20=[]


n_training_epochs=15

megarun_acc=[]
for m in range(n_runs):
  
  mean_acc_no20=[]

  for b in range(50):
    state=save_state_list[m]
    rng = jax.random.PRNGKey(b)
    rng, init_rng = jax.random.split(rng)
    train_ds, test_ds = get_datasets_unknown_allmnist(random_categories_test,seed=(b+20))
    for epoch in range(n_training_epochs):
      # Use a separate PRNG key to permute image data during shuffling
      rng, input_rng = jax.random.split(rng)
      # Run an optimization step over a training batch
      state = train_epoch(state, train_ds, batch_size, epoch, input_rng)
      # Evaluate on the test set after each training epoch 
    test_loss, test_accuracy = eval_model(state.params, test_ds)
    #print('test epoch: %d, loss: %.6f, accuracy: %.6f\n' % (          epoch, test_loss, test_accuracy * 100))

    if test_accuracy > 20 or test_accuracy < 20:
      #print("yuhu")
      mean_acc_no20.append(test_accuracy)
  #print("overall mean acc:", np.mean(mean_acc),"run:",m)
  print("overall mean_acc_no20:", np.mean(mean_acc_no20),"run:",m)

  print(len(mean_acc_no20))
  megarun_acc_no20.append(np.mean(mean_acc_no20))

train_ds["label"]

'''Few Shot learning gradient one network - relearn new network'''
learning_rate = 0.1
momentum = 0.9
batch_size=5

n_samples=25
mean_acc_no20=[]
megarun_acc_no20=[]

n_training_epochs=15

megarun_acc=[]
for m in range(n_runs):

  mean_acc=[]
  for b in range(50):
    rng = jax.random.PRNGKey(b)
  
    state = create_train_state(rng, learning_rate, momentum)

    rng = jax.random.PRNGKey(b)
    rng, init_rng = jax.random.split(rng)
    train_ds, test_ds = get_datasets_unknown_allmnist(random_categories_test,seed=(b+120))
    for epoch in range(n_training_epochs):
      # Use a separate PRNG key to permute image data during shuffling
      rng, input_rng = jax.random.split(rng)
      # Run an optimization step over a training batch
      state = train_epoch(state, train_ds, batch_size, epoch, input_rng)
      # Evaluate on the test set after each training epoch 
    test_loss, test_accuracy = eval_model(state.params, test_ds)
    print('test epoch: %d, loss: %.2f, accuracy: %.2f\n' % (          epoch, test_loss, test_accuracy * 100))
    if test_accuracy != 20.0:
      mean_acc_no20.append(test_accuracy)
  print("overall mean acc:", np.mean(mean_acc),"run:",m)
  print("overall mean_acc_no20:", np.mean(mean_acc_no20),"run:",m)
  megarun_acc.append(np.mean(mean_acc))
  megarun_acc_no20.append(np.mean(mean_acc_no20))

'''Few Shot learning gradient one network - relearn trained network'''
learning_rate = 0.1
momentum = 0.9
batch_size=5

n_samples=50
megarun_acc_no20=[]


n_training_epochs=15

megarun_acc=[]
for m in range(n_runs):
  
  mean_acc_no20=[]

  for b in range(50):
    state = create_train_state(rng, learning_rate, momentum)
    rng = jax.random.PRNGKey(b)
    rng, init_rng = jax.random.split(rng)
    train_ds, test_ds = get_datasets_unknown_allmnist(random_categories_test,seed=(b+20))
    for epoch in range(n_training_epochs):
      # Use a separate PRNG key to permute image data during shuffling
      rng, input_rng = jax.random.split(rng)
      # Run an optimization step over a training batch
      state = train_epoch(state, train_ds, batch_size, epoch, input_rng)
      # Evaluate on the test set after each training epoch 
    test_loss, test_accuracy = eval_model(state.params, test_ds)
    #print('test epoch: %d, loss: %.6f, accuracy: %.6f\n' % (          epoch, test_loss, test_accuracy * 100))

    if test_accuracy > 20 or test_accuracy < 20:
      #print("yuhu")
      mean_acc_no20.append(test_accuracy)
  #print("overall mean acc:", np.mean(mean_acc),"run:",m)
  print("overall mean_acc_no20:", np.mean(mean_acc_no20),"run:",m)

  print(len(mean_acc_no20))
  megarun_acc_no20.append(np.mean(mean_acc_no20))